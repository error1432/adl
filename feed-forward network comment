Theory: A Feed Forward Neural Network is an artificial neural network in which the connections between nodes does not form a cycle.
 The opposite of a feed forward neural network is a recurrent neural network, in which certain pathways are cycled. The feed forward model is the simplest form of
 neural network as information is only processed in one direction. While the data may pass through multiple hidden nodes, 
it always moves in one direction and never backwards. 
A Feed Forward Neural Network is commonly seen in its simplest form as a single layer perceptron. 
In this model, a series of inputs enter the layer and are multiplied by the weights. 
Each value is then added together to get a sum of the weighted input values. If the sum of the values is above a specific threshold, usually set at zero, 
the value produced is often 1, whereas if the sum falls below the threshold, the output value is -1. 

import tensorflow as tf
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

# Load Iris dataset
iris = load_iris()  # Loading Iris dataset into a variable.
X = iris.data  # Features of the dataset.
y = iris.target  # Class labels of the dataset.

# One-hot encode labels
lb = LabelBinarizer()  # Creating an instance of LabelBinarizer class for one-hot encoding.
y = lb.fit_transform(y)  # One-hot encoding the class labels.

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Splitting the dataset into training and testing sets with a test size of 20%.

# Define model architecture
model = tf.keras.Sequential([
    # First hidden layer with 16 neurons and input shape of 4 features. ReLU activation function is used.
    tf.keras.layers.Dense(16, input_shape=(4,), activation='relu'),
    # Second hidden layer with 8 neurons. ReLU activation function is used.
    tf.keras.layers.Dense(8, activation='relu'),
    # Output layer with 3 neurons for 3 classes. Softmax activation function is used for multiclass classification task.
    tf.keras.layers.Dense(3, activation='softmax')
])

# Compile model with different optimizers
optimizers = ['sgd', 'adam', 'rmsprop']  # List of optimizers to be used for training the model.
for optimizer in optimizers:  # Looping over each optimizer.
    # Compiling the model with 'categorical_crossentropy' as the loss function,
    # the current optimizer, and accuracy as the metric to be calculated.
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    # Train model
    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)
    # Training the model for 50 epochs with verbose=0 to suppress the output.

    # Evaluate model
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    # Evaluating the model on the test set and calculating the loss and accuracy.
    print('Optimizer:', optimizer)  # Printing the optimizer name.
    print('Test loss:', loss)  # Printing the loss value on the test set.
    print('Test accuracy:', accuracy)  # Printing the accuracy value on the test set.

# Allow the user to input values for the flower attributes
print('\nInput values for the flower attributes:')
sepal_length = float(input('Sepal length (cm): '))
sepal_width = float(input('Sepal width (cm): '))
petal_length = float(input('Petal length (cm): '))
petal_width = float(input('Petal width (cm): '))

# Predict the class of the flower based on input values
input_values = np.array([[sepal_length, sepal_width, petal_length, petal_width]])
prediction = model.predict(input_values)
predicted_class = np.argmax(prediction)
class_names = iris.target_names
print('\nPredicted class:', class_names[predicted_class])
