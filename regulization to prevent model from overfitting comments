
Regularization is a technique which makes slight modifications to the learning 
algorithm such that the model generalizes better. This in turn improves the modelâ€™s performance on the 
unseen data as well. L1 and L2 are the most common types of regularization. These update the general cost
 function by adding another term known as the regularization term. 
Cost function = Loss (say, binary cross entropy) + Regularization term 
Due to the addition of this regularization term, the values of weight matrices decrease because it
 assumes that a neural network with smaller weight matrices leads to simpler models. Therefore, it will also
 reduce overfitting to quite an extent. However, this regularization term differs in L1 and L2. 	 
Here, lambda is the regularization parameter. It is the hyperparameter whose value is optimized for better results.
 L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero). 

import tensorflow as tf
# Load the data
(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()

# Preprocess the data
train_data = train_data.reshape((60000, 784)) / 255.0
test_data = test_data.reshape((10000, 784)) / 255.0
train_labels = tf.keras.utils.to_categorical(train_labels)
test_labels = tf.keras.utils.to_categorical(test_labels)

# Define the model architecture
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,), kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(train_data, train_labels, epochs=10, batch_size=128, validation_data=(test_data, test_labels))
